# Observability Systems - Complete Picture

Build comprehensive understanding of observability systems, architecture, and UX patterns.

## 1. Observability Fundamentals

#### what is observability?

Observability is the capablity of system to allow enginers to understand its internal state based on the data its produced

![observablity](resource/screenshots/image.png)

The pillars of observability is Metrics, Logs, Traces

### Metrics

Metrics are numerical measurements recorded over a time that help us to understand how a system performing

#### Metrics Data contains:

- name : name of the process `process_count`
- value: number of times its counted like `20` request happen
- timestamp: at what time its computed
- labels: its represents in key value pair line `method="GET"`

`api_requests_total{status="200", method="GET"} TIMESTAMP VALUE`

They are best when we want to measure and react fast

- Observing the system performance that can be CPU,Memory usage,
- For setting the alerts (when its exceed the threshold limit)

It helps us to answer the following questions

- Do we need to scale up when its meet the threshold
- Is CPU is working high on the server
- Is the number of failed request increased
- Is API getting slower

#### Types of Metric Format

1. **counter**
1. **Guage**
1. **Histogram**
1. **Summary**

**1. Counter** : As name suggest its used for counting. It only increase and never go down and resets when the system resets (Odometer in bike)

- **Eg:** how many time api request is recived, how many are requested are failed

**2. Guage:** Its like a speedometer in bike that fluctuate. Its value go's up and down

- **Eg:** CPU consumpstion, memory usage by a process

**3. Histogram:** Its like a bar Table in real world analogy where each bar(range like 10-20,20-30) width is repsented as a bucket and the bar height represent the number of occurances happend in that range and it also contains total count and accumulated sum

- **Eg:**: success request in 100ms , 200ms, 100-200ms, as while as failure requests

**Summary:** Its similar to histogram without a bucket, Instead of showing counts for each range, it directly tells you how long it took for a certain percentage of requests to complete.

- **Eg:** p95 means - 95% of requests completed within Y milliseconds.

---

### Logs

Log are texts records generated by the applictioin that describe what happend at a specific moment

#### Log Data contains:

- TimeStamp: When the event happens
- Log Level: What is the type of error like DEBUG, WARN, INFO, ERROR
- Message: Description of what happend like "user login successful"
- Metadata : Its optional, it contains the userID, proceesID, IP address

#### When to use logs?

- When end user facing an issue in appliction
- when we want to know what happend before system crash
- when we want to trace the user actions
- When system throw an error

**_log_** example:
`INFO 2025-10-10T10:21:00Z User login successful {"userId": 57, "ip": "192.168.1.2"}`

#### Types of Log Format

1. **Unstructured Logs**
2. **Structured Logs**

**1. Unstructured Logs** : they are written in plain text which is human readble but hard for machines to figure out.

- **Eg:** `WARN 2025-10-10T10:21:00Z User login suspect {"userId": 57, "ip": "192.168.1.2"}`

**2. Structured Logs** : they are written in JSON format mostly which is human readble and machines also easily understand.

```
{
  "timestamp": "2025-11-19T10:21:00Z",
  "level": "INFO",
  "message": "User login successful",
  "userId": 57,
  "ip": "192.168.1.2",
  "service": "auth-service"
}
```

### Traces

Traces shows the journey of the single request that flows through diffrent services in the system

![Traces Rough Dig](resource/screenshots/traces.png)

#### Trace Data contains:

1. **Operation name**: Name of the operation like payment.verify
1. **start and end timestamp:**: start and end timestamp of the operation
1. **Duration:**: Duration for setteling the operation
1. **Span ID and Trace ID:**: span ID is unique for each step while Trace Id is same for all the steps
1. **Attributes:** it contains the DB query, userID, ..etc

#### When to use Trace?

- When our request is passing threw multiple services
- when we want to trace how much lateny a service is taking
- when we want to debug and find where the issue is happening

#### Concepts in Tracing

1. Spans
1. Trace Context
1. Baggage

**1. Spans:**
A span is a single step in a request Journey. each service creates a new span

**2. Trace Context:**
its a metadata that is needed to continue a trace across services. it has traceID, spanID, sampled

**3. Baggage:** It holds custom data that travels a across services during a trace

### Common protocols:

OTLP, Prometheus exposition format, StatsD, Syslog, Jaeger Thrift

1. **OTLP(OpenTelemetry Protocol)**: Its a modren protocal for sending the metrics, logs, traces. its Highly effcient and its standard

1. **Prometheus Exposition Format**: Its a text based format used for exporting metrics.

1. **Syslog**: Its used for transmitting logs

### Cardinality:

its the number of unique combinations of metric labels like (userId,orderID)

`api_requests_total{status="200", method="GET"}
`
this has 2 labels , 1 time series therfore low cardinality but if we add some more labels and there are millions users then then million timeseries then its high cardinality

Using ID(userid,orderId), timestamps,urls with params, ip address cause the Cardinality Explosion

High cardinality increases the memory usage, CPU usage, System crashes in Prometheus

### Time-Series Nature of Observability Data

Time-series everywhere in onservablity. Data is recorded as pairs (timestamp, value)

its helps to find out the spikes, patterns, latency, changes

**metrics:** `http_requests_total{status="200"} (count timestamp )`

**Logs:** each logs has a timesstamp
`logLevel TimeStamp message`

**Traces:** each span has a start and end timesstamp
`Span: payment.verify
Start: 10:22:00
End:   10:22:180`

![observabilty](resource/screenshots/observabilty-flow.png)

## 2. Data Collection Architecture

### What is a collector

A collector is an Agent that collect telementory data which is Log,metrics,traces before sending it to the backend observablity

#### Why do we need it instead of directly sending to backend

While the observablity server is running down and if we are trying sending data directly to the observablity its will take time to respond so its going to effect our main application but if we use collector agent the appliction will send its telementory data to local agent and the local agent works asyncronously to send data to backend and also performs protocal transform to standard **OTLP** amd do
retries.

### Collection models

1. Agent-based collection

   - we install an agent process on VM, server.
   - the agent works locally and handles all telementory related work
   - its work even backend is unreachable
   - performs batching so its decrease load on the backend
   - it can process, filter telememtory before sending

   1. its uses more CPU and Memory
   2. its requires installation and updation for newer version

2. Agentless Collection

   - No local agent running on the server
   - no installation needed
   - its good for serverless (
     cloud providers
     )
   - cloud providers collecting AWS Lambda logs via CloudWatch

   1. harder for debuging
   2. more latency as it need to pass threw multiple cloud services

3. eBPF-Based Collection

   - is a Linux kernel technology that allows running programs inside the kernel without modifying kernel code

   - its doesnt require instrumentation

   - its works with any language

   - it can observe function calss, network calls, system calls, events.

   - low overhead as its runs direcly in kernel

   1. its require Linux kernel ≥ 5.X
   2. harder to debug
   3. Not available in windows

### Instrumentation

Instrumentation means adding logic(code) to your application to collect telemetry data

#### Manual Instrumentation

Addind logic manually to collect the telemetry data

- it gives us full control
- We can able to collect whichever the data we needed
- Unique logs that no library can auto-generate
- we can write our custom logic

1. its need time and work force
2. Sometimes oversight can be happen

#### Auto-Instrumentation

Instrumentation added automatically by a library/agent without modifying your application code

- it overide HTTP request
- Its wrap around DB Query
- it can listen incoming
  and outgoing HTTP trace
- It need less workforce

1. Custom logic is missing
2. it cn increase the overhead by adding the data that we dont require

#### Push Vs Pull Models

1. **Push Model**

   - A serivce push telemetry data to a collector or backend at regular intervals

   ```
   App -> (push at regualar interval) -> collector/Backend
   ```

   - when the data need to send to different applications
   - when its need to perform retries and buffering, if the backend is down.
     -frontend and serverless

2. **Pull Model**

   - the backend scrapes the source endpoint to retrive telemetry

   - when we want to avoid agent installation
   - easy to know whether the service is down or not

### processing happens at collection point

#### Filtering Unwanted Data

Remove telemetry that is noisy, too expensive and not needed

its svaes storagecost, reduce network bandwidth, reduces noise so alerts are meaninfful

#### Sampling Strategies

Sampling means reducing the volume of data by following one of the Strategies.

1. **Head-based Sampling :**
   Decision made at the beginning of the request. Its good for low latency.low overhead but may miss errors requests
   eg: Sampling 1% of incoming requests

2. **Tail-Based Sampling :**
   Decision made after the trace is complete.
   collector waits and send the intresting traces

3. **Probabilistic Sampling :**
   Use probability logic (e.g., 10%) for consistent sampling. its good for large data

#### Buffering and Batching

**Buffering** is storing the telemetry data temporarily like sending data after collecting 100 log lines

**Batching** is sending the data at regular interval of time like sending the logs collected for every 10 sec

#### Metadata Enrichment(Tags, Labels, Context)

collector adds additional data automatically like userId, env, region, serviceName so that it makes telemetry searchable, filterable

#### Protocol Translation

Different tools use different protocols. Collector can convert one protocol to another standardize format like `StatsD → OTLP`

### Collection at different layers:

#### Application layer (instrumented code)

Using liberary and manual instrumentation at the code base level to emit the telementory data, like custom matrics , traces, application logs

#### System layer (host metrics, system logs)

Using Node exported,Windows performance counters to emit the system logs like CPU, Memory usage, Disk IO, System logs

#### Network layer (packet capture)

Using Service mesh telemetry, Sidecars, Sidecars to emit the betwork logs like paket loss, request latency, connection error.

#### Kernel layer (eBPF)

Using eBPF programs to collect kernel level termentory data like process-level telemetry,
network events, detailed performance data

![architecture of collection](resource/screenshots/architecture-of-collection.png)

---

## Backend Pipeline Architecture
